import 'dart:async';

import 'package:vit_gpt_dart_api/vit_gpt_dart_api.dart';
import 'package:vit_gpt_flutter_api/features/repositories/audio/players/vit_audio_player.dart';

import '../../data/enums/chat_status.dart';
import '../../factories/logger.dart';
import '../../features/usecases/audio/get_audio_intensity.dart';
import '../../features/usecases/get_error_message.dart';
import '../../features/usecases/platform/vibration/has_vibrator.dart';
import '../../features/usecases/platform/vibration/vibrate.dart';

class VoiceModeProvider {
  final void Function() notifyListeners;
  final bool Function() isVoiceMode;
  final void Function(ChatStatus status) setStatus;
  final void Function(String context, String message) errorReporter;
  final ChatStatus Function() getStatus;
  final Future<void> Function({
    required String text,
    required void Function(String chunk) onChunk,
  }) send;

  VoiceModeProvider({
    required this.notifyListeners,
    required this.isVoiceMode,
    required this.setStatus,
    required this.getStatus,
    required this.send,
    required this.errorReporter,
  }) {
    unawaited(_setup());
  }

  bool canVibrate = false;

  final voiceRecorder = VoiceRecorderHandler();

  /// Handler used in voice mode, to speake sentences generated by the AI model.
  SpeakerHandler? _speaker;

  /// A Stream of volumes either from the microphone or speaker, depending
  /// if the providder is listening from the user, or the AI is speaking.
  Stream<double>? audioVolumeStream;

  /// Necessary because `ChatStatus.sendingPrompt` and `ChatStatus.answering`
  /// does not tell if voice mode is on.
  ///
  /// This variable is not always true for voice mode process, it is just a
  /// helper variable. Use [isVoiceMode] for this check.
  bool isInVoiceMode = false;

  void dispose() {
    unawaited(voiceRecorder.dispose());
  }

  Future<void> _setup() async {
    await hasVibrator().then((v) => canVibrate = v);
    var micSendMode = await getMicSendMode();
    voiceRecorder.enableSilenceDetection =
        micSendMode == MicSendMode.intensitySilenceDetection;
  }

  Future<void> startVoiceMode() async {
    if (isVoiceMode()) {
      return;
    }
    await _listenToUser();
  }

  Future<void> _getVoiceResponse(String input) async {
    logger.info('Geting voice response');

    // For voice generation
    var speaker = await SpeakerHandler.fromLocalStorage();
    _speaker = speaker;
    audioVolumeStream = speaker.getVolumeStream();
    speaker.speakSentences();

    isInVoiceMode = true;
    notifyListeners();

    // Get response
    await send(
      text: input,
      onChunk: (chunk) async {
        var chatStatus = getStatus();
        if (chatStatus == ChatStatus.idle) {
          logger.warn('Disposing speaker on chunk receive');
          speaker.dispose();
          return;
        }
        if ([ChatStatus.listeningToUser, ChatStatus.transcribing]
            .contains(chatStatus)) {
          return;
        }
        await speaker.process(chunk);
        if (speaker.hasPendingSpeaches) {
          setStatus(ChatStatus.answeringAndSpeaking);
          // notifyListeners();
        }
      },
    );

    isInVoiceMode = false;

    // Checking if speaking was cancelled.
    if ([ChatStatus.listeningToUser, ChatStatus.transcribing]
        .contains(getStatus())) {
      return;
    }

    setStatus(ChatStatus.speaking);
    notifyListeners();

    Timer.periodic(const Duration(milliseconds: 500), (timer) async {
      if (speaker.hasPendingSpeaches) {
        logger.warn('Has pending speaches');
        return;
      }
      speaker.dispose();
      timer.cancel();
      logger.info('Finished speaking a response!');

      while (speaker.isSpeaking) {
        await Future.delayed(const Duration(milliseconds: 200));
      }

      if (isVoiceMode()) {
        if ([ChatStatus.listeningToUser, ChatStatus.transcribing]
            .contains(getStatus())) {
          return;
        }
        await _listenToUser();
      }
    });
  }

  Future<void> _stopListening() async {
    try {
      logger.info('Stop user listening');
      if (!voiceRecorder.isRecording) {
        return;
      }

      _executeListenEndFeedback();

      // Transcribe to create self message
      setStatus(ChatStatus.transcribing);
      notifyListeners();
      var file = await voiceRecorder.stop();
      var input = await transcribe(file);
      logger.info('Transcription: $input');

      // Sending to model
      setStatus(ChatStatus.thinking);
      audioVolumeStream = null;
      notifyListeners();
      await _getVoiceResponse(input);
    } on Exception catch (e) {
      var error = getErrorMessage(e);
      errorReporter('Transcrição', error ?? '');
      isInVoiceMode = false;
      setStatus(ChatStatus.idle);
    }
  }

  void _executeListenEndFeedback() {
    var player = VitAudioPlayer(
      audioPath: 'assets/audios/thinking.mp3',
    );
    unawaited(player.play());

    if (canVibrate) unawaited(vibrate());
  }

  Future<bool> _listenToUser() async {
    logger.info('Listening to user');
    setStatus(ChatStatus.listeningToUser);
    var isListening = await voiceRecorder.start();
    if (!isListening) {
      logger.warn('Unable to listen to user');
      return false;
    }
    notifyListeners();
    var amplitudes = voiceRecorder.rawAmplitudes.map((x) {
      return getAudioIntensity(value: x);
    });
    if (voiceRecorder.enableSilenceDetection) {
      logger.info('Listing for silence changes...');
      voiceRecorder.silenceStream.listen((silence) {
        logger.info('Silence $silence');
        if (silence) {
          _stopListening();
        }
      });
    }

    audioVolumeStream = amplitudes;
    notifyListeners();
    return true;
  }

  Future<void> stopVoiceMode() async {
    if (!isVoiceMode()) {
      logger.warn('Stopping voice mode aborted since voice mode is not active');
      notifyListeners();
      return;
    }
    logger.info('Stopping voice mode');
    audioVolumeStream = null;
    isInVoiceMode = false;
    setStatus(ChatStatus.idle);
    _speaker?.dispose();
    _speaker = null;
    try {
      if (voiceRecorder.isRecording) {
        await voiceRecorder.stop();
      }
    } finally {}
    notifyListeners();
  }

  /// If the provider is listening to the microphone, it stops recording.
  ///
  /// If the provider is speaking sentences from the AI, it stop speaking them.
  Future<void> stopVoiceInteraction() async {
    var status = getStatus();

    switch (status) {
      case ChatStatus.listeningToUser:
        await _stopListening();
      case ChatStatus.answeringAndSpeaking:
      case ChatStatus.speaking:
        logger.info('Stopped AI speaking');
        _speaker?.dispose();
        _speaker = null;
      // if (isVoiceMode()) {
      //   await _listenToUser();
      // }
      default:
        break;
    }
  }
}
